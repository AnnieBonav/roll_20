{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./output_chapters/\"\n",
    "chapters_file_name = \"chapters.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_file_path = output_folder + chapters_file_name\n",
    "\n",
    "# Confirm that file exists\n",
    "if not os.path.exists(chapters_file_path):\n",
    "    print(f\"Error: The file {chapters_file_path} does not exist.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"File {chapters_file_path} exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = {}\n",
    "# read the json and save it as dictionary\n",
    "with open(chapters_file_path, 'r') as file:\n",
    "    try:\n",
    "        chapters = json.load(file)\n",
    "        print(\"JSON file loaded successfully.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chapters: {len(chapters)}\")\n",
    "# Get the chapter called \"Introduction\"\n",
    "introduction_chapter = chapters[\"introduction\"]\n",
    "\n",
    "print(f\"Introduction chapter:\\n{introduction_chapter.get('content', '')[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113dac3",
   "metadata": {},
   "source": [
    "### That is ugly\n",
    "As you can see, we have data...but it is raw. We now need to divide it, but how do we do so?\n",
    "### Things to consider:\n",
    "- What is a line?  \n",
    "- How do we define where it starts and ends?  \n",
    "- What happens with sentences that -  \n",
    "finish in different lines?\n",
    "- Or sentences that start in one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb17ca1",
   "metadata": {},
   "source": [
    "page and then finish in another one?\n",
    "- What punctuation do we use?  \n",
    "- _Why is a sentence?_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_chapter_text = introduction_chapter.get(\"content\", \"\")\n",
    "print(f\"Raw chapter text length: {len(raw_chapter_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n lines using line breaks\n",
    "def split_text_by_lines(text, lines_per_chunk=5):\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk = \"\\n\".join(lines[i:i+lines_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // lines_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_line_jump = split_text_by_lines(raw_chapter_text, lines_per_chunk=1) # Using 1 line per chunk for clarity\n",
    "df_line_jump = pd.DataFrame(chunks_by_line_jump)\n",
    "df_line_jump.to_csv(output_folder + \"lines_chunk.csv\", index=False)\n",
    "print(df_line_jump.head(10))  # Display the first 10 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n sentences, using line range as the chunk key\n",
    "def split_text_by_sentences(text, sentences_per_chunk=5):\n",
    "    import re\n",
    "    # Replace line breaks with spaces to avoid breaking sentences\n",
    "    clean_text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # Split by period, question mark, or exclamation mark followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = {}\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk_sentences = sentences[i:i+sentences_per_chunk]\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "        # Key as \"start-end\" (0-based index)\n",
    "        key = f\"{i}-{i+len(chunk_sentences)-1}\"\n",
    "        chunks[key] = chunk_text\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702835b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentence chunks with sentence range as key\n",
    "chunks_by_sentence = split_text_by_sentences(text=raw_chapter_text, sentences_per_chunk=1)  # or >1 for multi-sentence chunks\n",
    "\n",
    "# Save as JSON: key = sentence range, value = chunk text\n",
    "output_json_path = os.path.join(output_folder, \"sentences_chunks.json\")\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_by_sentence, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sentence chunks JSON saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce7600",
   "metadata": {},
   "source": [
    "It is still not perfect. For example, if you mention \"A. Bonavides,\" it might be incorrectly split into two sentences. Or, you may want to save more information, such as grouping text by chapter (e.g., \"Ability Score Increase\"), which requires more advanced, semantic division of the PDF.  \n",
    "But to get started, this approach is more than enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n sentences, avoiding splits at abbreviations/initials\n",
    "def split_text_by_sentences_reg(text, sentences_per_chunk=5):\n",
    "    import re\n",
    "\n",
    "    # List of common abbreviations and initials to protect\n",
    "    abbreviations = [\n",
    "        r'(?:[A-Z]\\.){2,}',         # e.g., \"U.S.\", \"A.B.\"\n",
    "        r'(?:Mr|Mrs|Ms|Dr|Prof|Sr|Jr|St|vs|etc|e\\.g|i\\.e)\\.',  # common abbreviations\n",
    "    ]\n",
    "    # Protect abbreviations by replacing the period with a placeholder\n",
    "    protected_text = text\n",
    "    for abbr in abbreviations:\n",
    "        protected_text = re.sub(abbr, lambda m: m.group(0).replace('.', '<DOT>'), protected_text)\n",
    "\n",
    "    # Replace line breaks with spaces to avoid breaking sentences\n",
    "    clean_text = re.sub(r'\\s*\\n\\s*', ' ', protected_text)\n",
    "    # Split by period, question mark, or exclamation mark followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "    # Restore protected periods\n",
    "    sentences = [s.replace('<DOT>', '.') for s in sentences]\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = \" \".join(sentences[i:i+sentences_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // sentences_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4efcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_sentence_reg = split_text_by_sentences_reg(text=raw_chapter_text, sentences_per_chunk=1) # Using 1 sentence per chunk for clarity\n",
    "df_sentences_reg = pd.DataFrame(chunks_by_sentence_reg)\n",
    "\n",
    "# Show all rows and columns, and prevent text truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(df_sentences_reg.head(10))  # Display the first 10 chunks by sentences\n",
    "df_sentences_reg.to_csv(output_folder + \"sentences_chunks.csv\", index=False)\n",
    "\n",
    "# Return settings to default\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029ea50",
   "metadata": {},
   "source": [
    "### Now actually get a chapter to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = chapters[\"2\"]\n",
    "chapter_raw_text = chapter.get(\"content\", \"\")\n",
    "chapter_title = chapter.get(\"title\", \"No Title\")\n",
    "print(f\"{chapter_title} content:\\n{chapter_raw_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_in_chunk = 10\n",
    "chunks_by_sentence = split_text_by_sentences(text=chapter_raw_text, sentences_per_chunk=sentences_in_chunk)\n",
    "\n",
    "# Save as JSON: key = sentence range, value = chunk text\n",
    "output_json_path = os.path.join(output_folder, \"sentences_chunks.json\")\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_by_sentence, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sentence chunks JSON saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sentence chunks: {len(chunks_by_sentence)}\")\n",
    "\n",
    "# Get the average length of the chunks\n",
    "average_length = sum(len(chunk) for chunk in chunks_by_sentence.values()) / len(chunks_by_sentence)\n",
    "\n",
    "# Get min and max length of chunks\n",
    "min_length = min(len(chunk) for chunk in chunks_by_sentence.values())\n",
    "max_length = max(len(chunk) for chunk in chunks_by_sentence.values())\n",
    "\n",
    "print(f\"Average chunk length: {average_length:.2f} characters\")\n",
    "print(f\"Min chunk length: {min_length} characters\")\n",
    "print(f\"Max chunk length: {max_length} characters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roll_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
