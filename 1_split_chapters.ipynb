{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./output_chapters/\"\n",
    "chapters_file_name = \"chapters.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_file_path = output_folder + chapters_file_name\n",
    "\n",
    "# Confirm that file exists\n",
    "if not os.path.exists(chapters_file_path):\n",
    "    print(f\"Error: The file {chapters_file_path} does not exist.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"File {chapters_file_path} exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = {}\n",
    "# read the json and save it as dictionary\n",
    "with open(chapters_file_path, 'r') as file:\n",
    "    try:\n",
    "        chapters = json.load(file)\n",
    "        print(\"JSON file loaded successfully.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chapters: {len(chapters)}\")\n",
    "# Get the chapter called \"Introduction\"\n",
    "introduction_chapter = chapters[\"introduction\"]\n",
    "\n",
    "print(f\"Introduction chapter:\\n{introduction_chapter.get('content', '')[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113dac3",
   "metadata": {},
   "source": [
    "### That is ugly\n",
    "As you can see, we have data...but it is raw. We now need to divide it, but how do we do so?\n",
    "### Things to consider:\n",
    "- What is a line?  \n",
    "- How do we define where it starts and ends?  \n",
    "- What happens with sentences that -  \n",
    "finish in different lines?\n",
    "- Or sentences that start in one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb17ca1",
   "metadata": {},
   "source": [
    "page and then finish in another one?\n",
    "- What punctuation do we use?  \n",
    "- _Why is a sentence?_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_chapter_text = introduction_chapter.get(\"content\", \"\")\n",
    "print(f\"Raw chapter text length: {len(raw_chapter_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n lines using line breaks\n",
    "def split_text_by_lines(text, lines_per_chunk=5):\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk = \"\\n\".join(lines[i:i+lines_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // lines_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_line_jump = split_text_by_lines(raw_chapter_text, lines_per_chunk=1) # Using 1 line per chunk for clarity\n",
    "df_line_jump = pd.DataFrame(chunks_by_line_jump)\n",
    "df_line_jump.to_csv(output_folder + \"lines_chunk.csv\", index=False)\n",
    "print(df_line_jump.head(10))  # Display the first 10 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n sentences, saving extra info for each chunk\n",
    "def split_text_by_sentences(text, sentences_per_chunk=5):\n",
    "    import re\n",
    "    # Replace line breaks with spaces to avoid breaking sentences\n",
    "    clean_text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # Split by period, question mark, or exclamation mark followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = {}\n",
    "    for idx, i in enumerate(range(0, len(sentences), sentences_per_chunk)):\n",
    "        chunk_sentences = sentences[i:i+sentences_per_chunk]\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "        key = f\"{i}-{i+len(chunk_sentences)-1}\"\n",
    "        chunks[key] = {\n",
    "            \"chunk_id\": idx,\n",
    "            \"range\": key,\n",
    "            \"text\": chunk_text,\n",
    "            \"num_characters\": len(chunk_text)\n",
    "        }\n",
    "    return clean_text, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702835b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentence chunks with sentence range as key\n",
    "clean_text, chunks_by_sentence = split_text_by_sentences(text=raw_chapter_text, sentences_per_chunk=1)  # or >1 for multi-sentence chunks\n",
    "\n",
    "# Save as JSON: key = sentence range, value = chunk text\n",
    "output_json_path = os.path.join(output_folder, \"sentences_chunks.json\")\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_by_sentence, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Sentence chunks JSON saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce7600",
   "metadata": {},
   "source": [
    "It is still not perfect. For example, if you mention \"A. Bonavides,\" it might be incorrectly split into two sentences. Or, you may want to save more information, such as grouping text by chapter (e.g., \"Ability Score Increase\"), which requires more advanced, semantic division of the PDF.  \n",
    "But to get started, this approach is more than enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d799b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n sentences, avoiding splits at abbreviations/initials\n",
    "def split_text_by_sentences_reg(text, sentences_per_chunk=5):\n",
    "    import re\n",
    "\n",
    "    # List of common abbreviations and initials to protect\n",
    "    abbreviations = [\n",
    "        r'(?:[A-Z]\\.){2,}',         # e.g., \"U.S.\", \"A.B.\"\n",
    "        r'(?:Mr|Mrs|Ms|Dr|Prof|Sr|Jr|St|vs|etc|e\\.g|i\\.e)\\.',  # common abbreviations\n",
    "    ]\n",
    "    # Protect abbreviations by replacing the period with a placeholder\n",
    "    protected_text = text\n",
    "    for abbr in abbreviations:\n",
    "        protected_text = re.sub(abbr, lambda m: m.group(0).replace('.', '<DOT>'), protected_text)\n",
    "\n",
    "    # Replace line breaks with spaces to avoid breaking sentences\n",
    "    clean_text = re.sub(r'\\s*\\n\\s*', ' ', protected_text)\n",
    "    # Split by period, question mark, or exclamation mark followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "    # Restore protected periods\n",
    "    sentences = [s.replace('<DOT>', '.') for s in sentences]\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = \" \".join(sentences[i:i+sentences_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // sentences_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4efcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_sentence_reg = split_text_by_sentences_reg(text=raw_chapter_text, sentences_per_chunk=1) # Using 1 sentence per chunk for clarity\n",
    "df_sentences_reg = pd.DataFrame(chunks_by_sentence_reg)\n",
    "\n",
    "# Show all rows and columns, and prevent text truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(df_sentences_reg.head(10))  # Display the first 10 chunks by sentences\n",
    "df_sentences_reg.to_csv(output_folder + \"sentences_chunks.csv\", index=False)\n",
    "\n",
    "# Return settings to default\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029ea50",
   "metadata": {},
   "source": [
    "### Now actually get a chapter to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = chapters[\"2\"]\n",
    "chapter_raw_text = chapter.get(\"content\", \"\")\n",
    "chapter_title = chapter.get(\"title\", \"No Title\")\n",
    "print(f\"{chapter_title} content:\\n{chapter_raw_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_in_chunk = 10\n",
    "clean_text, chunks_by_sentence = split_text_by_sentences(text=chapter_raw_text, sentences_per_chunk=sentences_in_chunk)\n",
    "\n",
    "\n",
    "chapter2_json = {\n",
    "    \"time_created\": pd.Timestamp.now().isoformat(),\n",
    "    \"title\": chapter_title,\n",
    "    \"sentences_in_chunk\": sentences_in_chunk,\n",
    "    \"total_characters\": len(clean_text),\n",
    "    \"total_chunks\": len(chunks_by_sentence),\n",
    "    \"content\": chunks_by_sentence\n",
    "}\n",
    "\n",
    "output_json_path = os.path.join(output_folder, \"chapter2_chunks_with_info.json\")\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chapter2_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Chapter 2 chunked JSON with info saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f24f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_total_characters(json_path):\n",
    "    \"\"\"\n",
    "    Verifies that the sum of all chunk character counts matches the 'total_characters' field in the JSON,\n",
    "    allowing up to 1% difference.\n",
    "    Prints the result and returns True if within tolerance, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total_characters = data.get(\"total_characters\")\n",
    "    content = data.get(\"content\", {})\n",
    "\n",
    "    # Sum the num_characters field for each chunk\n",
    "    sum_characters = sum(chunk[\"num_characters\"] for chunk in content.values())\n",
    "\n",
    "    print(f\"total_characters field: {total_characters}\")\n",
    "    print(f\"Sum of all chunk characters: {sum_characters}\")\n",
    "\n",
    "    if total_characters == 0:\n",
    "        print(\"❌ total_characters is zero, cannot compare.\")\n",
    "        return False\n",
    "\n",
    "    percent_diff = abs(total_characters - sum_characters) / total_characters\n",
    "\n",
    "    print(f\"Percent difference: {percent_diff:.4%}\")\n",
    "\n",
    "    if percent_diff <= 0.01:\n",
    "        print(\"✅ The total is within 1% of the sum of all chunks.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ The total differs by more than 1% from the sum of all chunks.\")\n",
    "        return False\n",
    "\n",
    "verify_total_characters(os.path.join(output_folder, \"chapter2_chunks_with_info.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_chunk_info(json_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and calculates additional statistics about the chunks.\n",
    "    Prints the min, max, and average number of characters per chunk.\n",
    "    Also prints which chunk number has the min and max.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    content = data.get(\"content\", {})\n",
    "    num_characters = [chunk[\"num_characters\"] for chunk in content.values()]\n",
    "    chunk_keys = list(content.keys())\n",
    "\n",
    "    if not num_characters:\n",
    "        print(\"No chunks found.\")\n",
    "        return\n",
    "\n",
    "    min_chars = min(num_characters)\n",
    "    max_chars = max(num_characters)\n",
    "    avg_chars = sum(num_characters) / len(num_characters)\n",
    "\n",
    "    min_idx = num_characters.index(min_chars)\n",
    "    max_idx = num_characters.index(max_chars)\n",
    "\n",
    "    print(f\"Minimum characters in a chunk: {min_chars} (chunk: {chunk_keys[min_idx]})\")\n",
    "    print(f\"Maximum characters in a chunk: {max_chars} (chunk: {chunk_keys[max_idx]})\")\n",
    "    print(f\"Average characters per chunk: {avg_chars:.2f}\")\n",
    "\n",
    "additional_chunk_info(os.path.join(output_folder, \"chapter2_chunks_with_info.json\"))\n",
    "# This is fine cause 1000 characters are around 250 tokens, which is a good size for LLMs even if we send multiple chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fab2a3",
   "metadata": {},
   "source": [
    "This is a great example on how adding the regex for the knwon acronyms is great :)\n",
    "```\n",
    "\"550-559\": {\n",
    "    \"chunk_id\": 55,  \n",
    "    \"range\": \"550-559\",  \n",
    "    \"text\": \"line (Dex. save) Brass Fire 5 by 30 ft. line (Dex. save) Bronze Lightning 5 by 30 ft. line (Dex. save) Copper Acid 5 by 30 ft. line (Dex. save) Cold Fire 15 ft. cone (Dex. save) Green Poison 15 ft.\",  \n",
    "    \"num_characters\": 197  \n",
    "},\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roll_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
