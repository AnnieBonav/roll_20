{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb002ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./output_chapter/\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "chapter_file_name = \"chapter_chunks_with_info.json\"\n",
    "chapter_file_path = f\"./output_chapters/{chapter_file_name}\"\n",
    "\n",
    "if not os.path.exists(chapter_file_path):\n",
    "    print(f\"File {chapter_file_path} does not exist.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "with open(chapter_file_path, 'r') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_chunk_info(json_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and calculates additional statistics about the chunks.\n",
    "    Prints the min, max, and average number of characters per chunk.\n",
    "    Also prints which chunk number has the min and max.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    content = data.get(\"content\", {})\n",
    "    num_characters = [chunk[\"num_characters\"] for chunk in content.values()]\n",
    "    chunk_keys = list(content.keys())\n",
    "\n",
    "    if not num_characters:\n",
    "        print(\"No chunks found.\")\n",
    "        return\n",
    "\n",
    "    min_chars = min(num_characters)\n",
    "    max_chars = max(num_characters)\n",
    "    avg_chars = sum(num_characters) / len(num_characters)\n",
    "\n",
    "    min_idx = num_characters.index(min_chars)\n",
    "    max_idx = num_characters.index(max_chars)\n",
    "\n",
    "    print(f\"Minimum characters in a chunk: {min_chars} (chunk: {chunk_keys[min_idx]})\")\n",
    "    print(f\"Maximum characters in a chunk: {max_chars} (chunk: {chunk_keys[max_idx]})\")\n",
    "    print(f\"Average characters per chunk: {avg_chars:.2f}\")\n",
    "\n",
    "additional_chunk_info(json_path=chapter_file_path)\n",
    "# This is fine cause 1000 characters are around 250 tokens, which is a good size for LLMs even if we send multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "client = OpenAI()\n",
    "client.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05208d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is using openai but you could use any other library\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Returns the embedding vector for the given text\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = get_embedding(\"These are words\")\n",
    "print(f\"Embedding {embedded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embed the player question and save it\n",
    "player_question = \"What are my best traits as an elf?\"\n",
    "player_question_embedding = get_embedding(player_question)\n",
    "player_question_embedding_file_path = f\"{output_folder}player_question_embedding.json\"\n",
    "\n",
    "with open(f\"{player_question_embedding_file_path}\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"question\": player_question,\n",
    "        \"embedding\": player_question_embedding\n",
    "    }, f)\n",
    "\n",
    "print(\"Player question embedding saved to player_question_embedding.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embed each chunk and save to a new JSON file\n",
    "embeded_chunks_file_path = f\"{output_folder}embedded_chunks.json\"\n",
    "\n",
    "embedded_chunks = {}\n",
    "for key, chunk in data[\"content\"].items():\n",
    "    chunk_text = chunk[\"text\"]\n",
    "    embedding = get_embedding(chunk_text)\n",
    "    chunk_with_embedding = chunk.copy()\n",
    "    chunk_with_embedding[\"embedding\"] = embedding\n",
    "    embedded_chunks[key] = chunk_with_embedding\n",
    "\n",
    "with open(f\"{embeded_chunks_file_path}\", \"w\") as f:\n",
    "    json.dump(embedded_chunks, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Read the new JSON file and compare to find the 3 most similar chunks\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "with open(f\"{embeded_chunks_file_path}\", \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "with open(player_question_embedding_file_path, \"r\") as f:\n",
    "    player_q = json.load(f)\n",
    "    player_embedding = player_q[\"embedding\"]\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = []\n",
    "for key, chunk in loaded_chunks.items():\n",
    "    sim = cosine_similarity(player_embedding, chunk[\"embedding\"])\n",
    "    similarities.append((key, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and get top 3\n",
    "top_3 = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print(\"Top 3 most similar chunks:\")\n",
    "for key, sim in top_3:\n",
    "    print(f\"Chunk {key} (similarity: {sim:.4f}):\")\n",
    "    print(loaded_chunks[key][\"text\"][:300])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a96734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make those top 3 a single string\n",
    "top_3_chunks = \"\\n\\n\".join(\n",
    "    [loaded_chunks[key][\"text\"] for key, _ in top_3]\n",
    ")\n",
    "\n",
    "# 4. Save the top 3 chunks to a new txt file\n",
    "top_3_chunks_file_path = f\"{output_folder}top_3_chunks.txt\"\n",
    "with open(top_3_chunks_file_path, \"w\") as f:\n",
    "    f.write(top_3_chunks)\n",
    "print(f\"Top 3 chunks saved to {top_3_chunks_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13075077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top_3_chunks text from txt\n",
    "with open(top_3_chunks_file_path, \"r\") as f:\n",
    "    top_3_chunks_text = f.read()\n",
    "\n",
    "print(\"Top 3 chunks text:\")\n",
    "print(top_3_chunks_text[:300])  # Print first 300 characters for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc37e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_question_with_context = f'''\n",
    "You are a helpful assistant that provides answers based on the context of a fantasy world. You must use the provided context to answer the player's question. You MUST provide snippets from the context to support your answer. If you don't know the answer, say \"I don't know\" and do not make up information.\n",
    "Player Question: {player_question}\n",
    "Context: {top_3_chunks_text}\n",
    "'''\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    input=f\"{player_question_with_context}\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de43da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_player_question(question, client, data, output_folder, embedded_chunks_file_path):\n",
    "    \"\"\"\n",
    "    Embeds the player question, finds the top 3 most similar chunks using cosine similarity,\n",
    "    and generates an answer using the context of those chunks. Each question/answer session\n",
    "    is saved in its own folder for easy review.\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import datetime\n",
    "\n",
    "    # Create a unique session folder for this question\n",
    "    session_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + str(uuid.uuid4())[:8]\n",
    "    session_folder = os.path.join(output_folder, f\"session_{session_id}\")\n",
    "    os.makedirs(session_folder, exist_ok=True)\n",
    "\n",
    "    # 1. Embed the player question and save it\n",
    "    embedding = get_embedding(question)\n",
    "    question_embedding_file_path = os.path.join(session_folder, \"player_question_embedding.json\")\n",
    "    with open(question_embedding_file_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"question\": question,\n",
    "            \"embedding\": embedding\n",
    "        }, f)\n",
    "\n",
    "    # 2. Load embedded chunks\n",
    "    with open(embedded_chunks_file_path, \"r\") as f:\n",
    "        loaded_chunks = json.load(f)\n",
    "\n",
    "    # 3. Compute similarities\n",
    "    similarities = []\n",
    "    for key, chunk in loaded_chunks.items():\n",
    "        sim = cosine_similarity(embedding, chunk[\"embedding\"])\n",
    "        similarities.append((key, sim))\n",
    "    top_3 = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    # 4. Get top 3 chunks as context and save them\n",
    "    top_3_chunks = \"\\n\\n\".join(\n",
    "        [loaded_chunks[key][\"text\"] for key, _ in top_3]\n",
    "    )\n",
    "    top_3_chunks_file_path = os.path.join(session_folder, \"top_3_chunks.txt\")\n",
    "    with open(top_3_chunks_file_path, \"w\") as f:\n",
    "        f.write(top_3_chunks)\n",
    "\n",
    "    # 5. Prepare prompt and get answer\n",
    "    player_question_with_context = f'''\n",
    "You are a helpful assistant that provides answers based on the context of a fantasy world. You must use the provided context to answer the player's question. You MUST provide snippets from the context to support your answer. If you don't know the answer, say \"I don't know\" and do not make up information.\n",
    "Player Question: {question}\n",
    "Context: {top_3_chunks}\n",
    "'''\n",
    "    response = client.responses.create(\n",
    "        model=\"o4-mini-2025-04-16\",\n",
    "        input=player_question_with_context\n",
    "    )\n",
    "\n",
    "    # Save the answer\n",
    "    answer_file_path = os.path.join(session_folder, \"answer.txt\")\n",
    "    with open(answer_file_path, \"w\") as f:\n",
    "        f.write(response.output_text)\n",
    "\n",
    "    print(f\"Session saved in: {session_folder}\")\n",
    "    print(response.output_text)\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_question = \"How far do I see as a dwarf?\"\n",
    "with open(embeded_chunks_file_path, \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "answer_player_question(\n",
    "    question=player_question,\n",
    "    client=client,\n",
    "    data=loaded_chunks,\n",
    "    output_folder=output_folder,\n",
    "    embedded_chunks_file_path=embeded_chunks_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0940ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_question = \"How far do I see as a spider?\"\n",
    "with open(embeded_chunks_file_path, \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "answer_player_question(\n",
    "    question=player_question,\n",
    "    client=client,\n",
    "    data=loaded_chunks,\n",
    "    output_folder=output_folder,\n",
    "    embedded_chunks_file_path=embeded_chunks_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_question = \"If I want to be quicker, which race should I be?\"\n",
    "with open(embeded_chunks_file_path, \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "answer_player_question(\n",
    "    question=player_question,\n",
    "    client=client,\n",
    "    data=loaded_chunks,\n",
    "    output_folder=output_folder,\n",
    "    embedded_chunks_file_path=embeded_chunks_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c213167",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_question = \"If I want to be quicker, should I be a dwarf or an elf?\"\n",
    "with open(embeded_chunks_file_path, \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "answer_player_question(\n",
    "    question=player_question,\n",
    "    client=client,\n",
    "    data=loaded_chunks,\n",
    "    output_folder=output_folder,\n",
    "    embedded_chunks_file_path=embeded_chunks_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e233b3",
   "metadata": {},
   "source": [
    "### Now this is the fun part\n",
    "RAG is super powerful because it is super simple: get the context you need...but how do we know where it is what we need? Or even worse! How do we know what we need?\n",
    "\n",
    "#### Ideas\n",
    "We could create an agent that divides the question into questions that we should get context for each with RAG. Then we get context using each question and we have context for the whole answer (lets see how GPT does this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def agent_decompose_and_rag(question, client, data, output_folder, embedded_chunks_file_path):\n",
    "    \"\"\"\n",
    "    1. Uses the LLM to decompose a complex question into sub-questions.\n",
    "    2. For each sub-question, retrieves top-3 relevant chunks using RAG.\n",
    "    3. Aggregates all retrieved context and asks the LLM to answer the original question, citing context.\n",
    "    \"\"\"\n",
    "    # Step 1: Decompose the question\n",
    "    decompose_prompt = f\"\"\"\n",
    "You are an expert assistant. Given the following player question, break it down into the minimal set of sub-questions needed to fully answer it. Return the sub-questions as a Python list of strings.\n",
    "\n",
    "Player Question: {question}\n",
    "\"\"\"\n",
    "    decompose_response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": decompose_prompt}]\n",
    "    )\n",
    "    # Try to extract the list of sub-questions\n",
    "    try:\n",
    "        sub_questions = ast.literal_eval(decompose_response.choices[0].message.content.strip())\n",
    "    except Exception:\n",
    "        # fallback: treat as single question\n",
    "        sub_questions = [question]\n",
    "\n",
    "    # Step 2: For each sub-question, get top-3 context chunks\n",
    "    with open(embedded_chunks_file_path, \"r\") as f:\n",
    "        loaded_chunks = json.load(f)\n",
    "\n",
    "    all_contexts = []\n",
    "    for sub_q in sub_questions:\n",
    "        embedding = get_embedding(sub_q)\n",
    "        similarities = []\n",
    "        for key, chunk in loaded_chunks.items():\n",
    "            sim = cosine_similarity(embedding, chunk[\"embedding\"])\n",
    "            similarities.append((key, sim))\n",
    "        top_3 = sorted(similarities, key=lambda x: x[1], reverse=True)[:3]\n",
    "        context = \"\\n\\n\".join([loaded_chunks[key][\"text\"] for key, _ in top_3])\n",
    "        all_contexts.append(f\"Sub-question: {sub_q}\\nContext:\\n{context}\")\n",
    "\n",
    "    # Step 3: Aggregate all context and ask for a final answer\n",
    "    aggregated_context = \"\\n\\n\".join(all_contexts)\n",
    "    final_prompt = f\"\"\"\n",
    "You are a helpful assistant that answers player questions about a fantasy world. Use the provided context for each sub-question to answer the original question. Cite snippets from the context to support your answer. If you don't know, say \"I don't know\" and do not make up information.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Context for sub-questions:\n",
    "{aggregated_context}\n",
    "\"\"\"\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": final_prompt}]\n",
    "    )\n",
    "    print(final_response.choices[0].message.content)\n",
    "    return final_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3452e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeded_chunks_file_path, \"r\") as f:\n",
    "    loaded_chunks = json.load(f)\n",
    "\n",
    "agent_decompose_and_rag(\n",
    "    question=\"If I want to be quicker, should I be a dwarf or an elf?\",\n",
    "    client=client,\n",
    "    data=loaded_chunks,\n",
    "    output_folder=output_folder,\n",
    "    embedded_chunks_file_path=embeded_chunks_file_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roll_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
