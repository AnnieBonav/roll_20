{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d05fedc",
   "metadata": {},
   "source": [
    "# Demistifying RAG\n",
    "## And how I prove that it is way more dificult writing Demistifying than RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./resources/DNDPlayersHandbook_Races.pdf\"\n",
    "output_folder = \"./output/\"\n",
    "lines_per_chunk = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6e2af",
   "metadata": {},
   "source": [
    "## Reading and extracting test\n",
    "For this, you can really use any library/manual thing you want. It could also be the case that you have raw text/jsons/databases...So, as long as at the end there is raw data you can work with, you are good :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read PDF and extract text\n",
    "def pdf_to_text(pdf_path, start_page=0):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        # Loop from start_page to the end\n",
    "        for page in reader.pages[start_page:]:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b6c61",
   "metadata": {},
   "source": [
    "Depending on your extraction method you will get different results. A fancier program/library could give better results, which would make using the raw data easier. The next frame gives an example on using good'old PYPDF2 reader which, as you will see, gives AMAZING results for being a lightweight option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38234a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pdf_to_text(pdf_path=pdf_path)\n",
    "print(\"Raw text extracted from PDF.\\n{}\\n\".format(raw_text[:500]))  # Print first 500 characters\n",
    "# This is very cool because, as you can see, the different columns are respected. This text only contains the first column and nothing from \"Racial trates\" appears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b6c87",
   "metadata": {},
   "source": [
    "As you can see, we have data...but it is raw. We now need to divide it, but how do we do so?\n",
    "### Things to consider:\n",
    "- What is a line?  \n",
    "- How do we define where it starts and ends?  \n",
    "- What happens with sentences that -  \n",
    "finish in different lines?\n",
    "- Or sentences that start in one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034e779",
   "metadata": {},
   "source": [
    "page and then finish in another one?\n",
    "- What punctuation do we use?  \n",
    "- _Why is a sentence?_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n lines using line breaks\n",
    "def split_text_by_lines(text, lines_per_chunk=5):\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), lines_per_chunk):\n",
    "        chunk = \"\\n\".join(lines[i:i+lines_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // lines_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5762ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_line_jump = split_text_by_lines(raw_text, lines_per_chunk=1) # Using 1 line per chunk for clarity\n",
    "df_line_jump = pd.DataFrame(chunks_by_line_jump)\n",
    "df_line_jump.to_csv(output_folder + \"lines_chunk.csv\", index=False)\n",
    "print(df_line_jump.head(10))  # Display the first 10 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67baa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks of n sentences (using periods), handling line breaks\n",
    "def split_text_by_sentences(text, sentences_per_chunk=5):\n",
    "    import re\n",
    "    # Replace line breaks with spaces to avoid breaking sentences\n",
    "    clean_text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # Split by period, question mark, or exclamation mark followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = \" \".join(sentences[i:i+sentences_per_chunk])\n",
    "        chunks.append({\"chunk_id\": i // sentences_per_chunk + 1, \"text\": chunk})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_by_sentence = split_text_by_sentences(text=raw_text, sentences_per_chunk=1) # Using 1 sentence per chunk for clarity\n",
    "df_sentences = pd.DataFrame(chunks_by_sentence)\n",
    "\n",
    "# Show all rows and columns, and prevent text truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(df_sentences.head(10))  # Display the first 10 chunks by sentences\n",
    "df_sentences.to_csv(output_folder + \"sentences_chunks.csv\", index=False)\n",
    "\n",
    "# Return settings to default\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a069ec6",
   "metadata": {},
   "source": [
    "It is still not perfect. For example, if you mention \"A. Bonavides,\" it might be incorrectly split into two sentences. Or, you may want to save more information, such as grouping text by chapter (e.g., \"Ability Score Increase\"), which requires more advanced, semantic division of the PDF.  \n",
    "But to get started, this approach is more than enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_by_headings(text):\n",
    "    # Example: Heading is a line with all uppercase letters and at least 3 characters\n",
    "    pattern = r'(?m)^(?P<heading>[A-Z][A-Z\\s]{2,})$'\n",
    "    splits = [m.start() for m in re.finditer(pattern, text)]\n",
    "    splits.append(len(text))\n",
    "    chunks = []\n",
    "    for i in range(len(splits)-1):\n",
    "        chunk_text = text[splits[i]:splits[i+1]].strip()\n",
    "        if chunk_text:\n",
    "            chunks.append({\"chunk_id\": i+1, \"text\": chunk_text})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[:500])  # Print first 500 characters for context\n",
    "chunks_by_heading = split_text_by_headings(text=raw_text)\n",
    "df_headings = pd.DataFrame(chunks_by_heading)\n",
    "print(df_headings.head(10))  # Display the first 10 chunks by headings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2c0fe",
   "metadata": {},
   "source": [
    "## Splitting by chapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_chapters(text):\n",
    "    # Regex to match lines like \"Chapter 1: Title\" or \"Chapter 2 Title\"\n",
    "    pattern = r'(?im)^(Chapter\\s+(\\d+)[^\\n]*)$'\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "    chapters = []\n",
    "    for i, match in enumerate(matches):\n",
    "        chapter_line = match.group(1).strip()\n",
    "        chapter_num = match.group(2)\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        content = text[start:end].strip()\n",
    "        chapters.append({\n",
    "            \"chapter_number\": chapter_num,\n",
    "            \"chapter_title\": chapter_line,\n",
    "            \"content\": content\n",
    "        })\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065421e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_chapters(text):\n",
    "    import re\n",
    "    # Regex: \"chapter\" (with optional spaces), number, optional spaces, colon (with optional spaces), then title\n",
    "    # Allow optional non-word chars or digits before \"chapter\" (to handle cases like \"20 10 1Ch apter  5: E q u ipm en t\")\n",
    "    pattern = r'(?im)^.*?(?:[Cc]\\s*[Hh]\\s*[Aa]\\s*[Pp]\\s*[Tt]\\s*[Ee]\\s*[Rr])\\s*(\\d+)\\s*:\\s*[^\\n]*$'\n",
    "    matches = list(re.finditer(pattern, text, re.MULTILINE))\n",
    "    chapters = []\n",
    "    expected_chapter = 1\n",
    "    last_chapter = None\n",
    "\n",
    "    # Handle introduction (everything before first chapter)\n",
    "    if matches and matches[0].start() > 0:\n",
    "        intro_content = text[:matches[0].start()].strip()\n",
    "        if intro_content:\n",
    "            chapters.append({\n",
    "                \"chapter_number\": \"introduction\",\n",
    "                \"chapter_title\": \"Introduction\",\n",
    "                \"content\": intro_content\n",
    "            })\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        # Find the full matched line for the chapter title\n",
    "        line_start = text.rfind('\\n', 0, match.start()) + 1\n",
    "        line_end = text.find('\\n', match.start())\n",
    "        if line_end == -1:\n",
    "            line_end = len(text)\n",
    "        chapter_line = text[line_start:line_end].strip()\n",
    "        chapter_num = int(match.group(1))\n",
    "        # Check for sequential chapter numbers\n",
    "        if chapter_num != expected_chapter:\n",
    "            raise ValueError(\n",
    "                f\"Expected Chapter {expected_chapter} after Chapter {last_chapter}, but found Chapter {chapter_num}\"\n",
    "            )\n",
    "        last_chapter = chapter_num\n",
    "        expected_chapter += 1\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        content = text[start:end].strip()\n",
    "        chapters.append({\n",
    "            \"chapter_number\": str(chapter_num),\n",
    "            \"chapter_title\": chapter_line,\n",
    "            \"content\": content\n",
    "        })\n",
    "    return chapters\n",
    "# filepath: /Users/annie/dev/roll_20/roll_20.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'(?im)^((?:[Cc]\\s*)?(?:[Hh]\\s*)?(?:[Aa]\\s*)?(?:[Pp]\\s*)?(?:[Tt]\\s*)?(?:[Ee]\\s*)?(?:[Rr]\\s*)\\s*(\\d+)(?:\\s|:)[^\\n]*)$'\n",
    "test_text = \"\"\"\n",
    "355,000 20 +6\n",
    "\n",
    "Ch apter  2: R aces\n",
    "A  VISIT TO ONE OF TH\n",
    "\"\"\"\n",
    "\n",
    "matches = list(re.finditer(pattern, test_text, re.MULTILINE))\n",
    "for m in matches:\n",
    "    print(\"MATCH:\", m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_book_pdf_path = \"./resources/DNDPlayersHandbook.pdf\"\n",
    "all_book_raw_text = pdf_to_text(pdf_path=all_book_pdf_path, start_page=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ed3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the raw text to a file for reference\n",
    "with open(output_folder + \"all_book_raw_text.txt\", \"w\") as f:\n",
    "    f.write(all_book_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = split_text_by_chapters(all_book_raw_text)\n",
    "df_chapters = pd.DataFrame(chapters)\n",
    "df_chapters.to_csv(output_folder + \"chapters.csv\", index=False)\n",
    "print(df_chapters.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f05932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary: chapter number as key, value is dict with title and content\n",
    "chapters_dict = {\n",
    "    chapter[\"chapter_number\"]: {\n",
    "        \"title\": chapter[\"chapter_title\"],\n",
    "        \"content\": chapter[\"content\"]\n",
    "    }\n",
    "    for chapter in chapters\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_folder + \"chapters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chapters_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e640029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print the first two chapters for verification\n",
    "# print(json.dumps({k: chapters_dict[k] for k in list(chapters_dict)[:2]}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074df364",
   "metadata": {},
   "source": [
    "## The Two Main Parts of RAG\n",
    "\n",
    "When working with Retrieval-Augmented Generation (RAG), there are two main components to consider:\n",
    "\n",
    "1. **Structuring Your Information:**  \n",
    "    The first step is to ensure your data is organized in a way that makes it easy to retrieve and use. This involves cleaning, chunking, and formatting your information so that it can be efficiently searched and referenced.\n",
    "\n",
    "2. **Choosing What to Send:**  \n",
    "    Once your data is well-structured, the next challenge is deciding which pieces of information to send to your model or downstream process. This selection step is crucial for maximizing relevance and performance.\n",
    "\n",
    "---\n",
    "\n",
    "In the next section, I'll focus on strategies for choosing what information to send."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roll_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
